{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBs3jNavihqy96JFEPTUDK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irullah/prosaindata/blob/main/topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC MODELLING"
      ],
      "metadata": {
        "id": "mIArJ0JlUSMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHRtl2M3ULap"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from googleapiclient.discovery import build"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def video_comments(video_id):\n",
        "\t# empty list for storing reply\n",
        "\treplies = []\n",
        "\n",
        "\t# creating youtube resource object\n",
        "\tyoutube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "\t# retrieve youtube video results\n",
        "\tvideo_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()\n",
        "\n",
        "\t# iterate video response\n",
        "\twhile video_response:\n",
        "\t\t\n",
        "\t\t# extracting required info\n",
        "\t\t# from each result object\n",
        "\t\tfor item in video_response['items']:\n",
        "\t\t\t\n",
        "\t\t\t# Extracting comments ()\n",
        "\t\t\tpublished = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
        "\t\t\tuser = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
        "\n",
        "\t\t\t# Extracting comments\n",
        "\t\t\tcomment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "\t\t\tlikeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
        "\n",
        "\t\t\treplies.append([published, user, comment, likeCount])\n",
        "\t\t\t\n",
        "\t\t\t# counting number of reply of comment\n",
        "\t\t\treplycount = item['snippet']['totalReplyCount']\n",
        "\n",
        "\t\t\t# if reply is there\n",
        "\t\t\tif replycount>0:\n",
        "\t\t\t\t# iterate through all reply\n",
        "\t\t\t\tfor reply in item['replies']['comments']:\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Extract reply\n",
        "\t\t\t\t\tpublished = reply['snippet']['publishedAt']\n",
        "\t\t\t\t\tuser = reply['snippet']['authorDisplayName']\n",
        "\t\t\t\t\trepl = reply['snippet']['textDisplay']\n",
        "\t\t\t\t\tlikeCount = reply['snippet']['likeCount']\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Store reply is list\n",
        "\t\t\t\t\t#replies.append(reply)\n",
        "\t\t\t\t\treplies.append([published, user, repl, likeCount])\n",
        "\n",
        "\t\t\t# print comment with list of reply\n",
        "\t\t\t#print(comment, replies, end = '\\n\\n')\n",
        "\n",
        "\t\t\t# empty reply list\n",
        "\t\t\t#replies = []\n",
        "\n",
        "\t\t# Again repeat\n",
        "\t\tif 'nextPageToken' in video_response:\n",
        "\t\t\tvideo_response = youtube.commentThreads().list(\n",
        "\t\t\t\t\tpart = 'snippet,replies',\n",
        "\t\t\t\t\tpageToken = video_response['nextPageToken'], \n",
        "\t\t\t\t\tvideoId = video_id\n",
        "\t\t\t\t).execute()\n",
        "\t\telse:\n",
        "\t\t\tbreak\n",
        "\t#endwhile\n",
        "\treturn replies"
      ],
      "metadata": {
        "id": "iMQEkoOVNphF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# isikan dengan api key Anda\n",
        "api_key = 'AIzaSyDGy3ZfZu4UhoOyySakiB2TEu0Y9X4OmLw'\n",
        "\n",
        "# Enter video id\n",
        "# contoh url video = https://www.youtube.com/watch?v=5tucmKjOGi8\n",
        "video_id = \"KtntKGlmuZw\" #isikan dengan kode / ID video\n",
        "\n",
        "# Call function\n",
        "comments = video_comments(video_id)\n",
        "\n",
        "comments"
      ],
      "metadata": {
        "id": "KSj1Jkp_Nr6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(comments, columns=['Tanggal', 'Nama', 'Komen', 'Like'])\n",
        "df"
      ],
      "metadata": {
        "id": "MVcrz0_ZNtoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('youtube-comments.csv', index=False)"
      ],
      "metadata": {
        "id": "GRAMJy07Nvzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install library\n",
        "!pip install sastrawi\n",
        "!pip install swifter\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "vAsPjAmqNxvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "import re\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Z7vzx69NN0Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses menghilangkan simbol dan emoji\n",
        "def remove_text_special (text):\n",
        "  text = text.replace('\\\\t',\"\").replace('\\\\n',\"\").replace('\\\\u',\"\").replace('\\\\',\"\")\n",
        "  text = text.encode('ascii', 'replace').decode('ascii')\n",
        "  return text.replace(\"http://\",\" \").replace(\"https://\", \" \")\n",
        "df['Komen'] = df['Komen'].apply(remove_text_special)\n",
        "print(df['Komen'])"
      ],
      "metadata": {
        "id": "iFyRcYHJN2JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tanda_baca(text):\n",
        "  text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text)\n",
        "  return text\n",
        "\n",
        "df['Komen'] = df['Komen'].apply(remove_tanda_baca)\n",
        "df['Komen'].head(20)"
      ],
      "metadata": {
        "id": "Z-XVe6r0N35N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses menghilangkan angka\n",
        "def remove_numbers (text):\n",
        "  return re.sub(r\"\\d+\", \"\", text)\n",
        "df['Komen'] = df['Komen'].apply(remove_numbers)\n",
        "df['Komen']"
      ],
      "metadata": {
        "id": "D9hUUuFyN69T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses casefolding\n",
        "def casefolding(Comment):\n",
        "  Comment = Comment.lower()\n",
        "  return Comment\n",
        "df['Komen'] = df['Komen'].apply(casefolding)\n",
        "df['Komen']"
      ],
      "metadata": {
        "id": "1GdUjOsRN8sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses tokenisasi\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('punkt')\n",
        "# def word_tokenize(text):\n",
        "#   tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "#   return tokenizer.tokenize(text)\n",
        "\n",
        "df['review_token'] = df['Komen'].apply(lambda sentence: nltk.word_tokenize(sentence))\n",
        "df['review_token']"
      ],
      "metadata": {
        "id": "4mwiIb-1N-Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopword Removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "txt_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "def stopwords_removal(filtering) :\n",
        "  filtering = [word for word in filtering if word not in txt_stopwords]\n",
        "  return filtering\n",
        "\n",
        "df['stopwords_removal'] = df['review_token'].apply(stopwords_removal)\n",
        "df['stopwords_removal'].head(20)"
      ],
      "metadata": {
        "id": "AQwWOoBfN_Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses stem\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import string\n",
        "import swifter\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemming (term):\n",
        "  return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "for document in df['stopwords_removal']:\n",
        "  for term in document:\n",
        "    if term not in term_dict:\n",
        "      term_dict[term] = ''\n",
        "print(len(term_dict))\n",
        "print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "AVq7TvLQOEmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for term in term_dict:\n",
        "  term_dict[term] = stemming(term)\n",
        "  term,\":\",term_dict[term]\n",
        "\n",
        "print(term_dict)\n",
        "print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "ZuoQbL8GOG8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stemming(document):\n",
        "  return [term_dict[term] for term in document]\n",
        "df['stemming'] = df['stopwords_removal'].swifter.apply(get_stemming)\n",
        "\n",
        "print(df['stemming'])\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "Pum8khOROIxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word(data):\n",
        "  kalimat = \"\"\n",
        "  for i in data:\n",
        "    kalimat += i\n",
        "    kalimat += \" \"\n",
        "  return kalimat\n",
        "\n",
        "text = df['stemming'].swifter.apply(word)\n",
        "text"
      ],
      "metadata": {
        "id": "LFFLJ60uOMCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Vectorize document using TF-IDF\n",
        "tfidf = TfidfVectorizer(lowercase=True,\n",
        "                        stop_words='english',\n",
        "                        ngram_range = (1,1))\n",
        "\n",
        "# Fit and Transform the documents\n",
        "train_data = tfidf.fit_transform(text) \n",
        "print(train_data)"
      ],
      "metadata": {
        "id": "oJoaY8TFONgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tfidf = pd.DataFrame(\n",
        "    train_data.toarray().T, columns=[f'D{i+1}' for i in range(len(text))], index=tfidf.get_feature_names_out()\n",
        ")\n",
        "df_tfidf"
      ],
      "metadata": {
        "id": "XefDaJ3yOPQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of topics or components\n",
        "num_components=10\n",
        "\n",
        "# Create SVD object\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
        "\n",
        "# Fit SVD model on data\n",
        "lsa.fit_transform(train_data)\n",
        "\n",
        "# Get Singular values and Components \n",
        "Sigma = lsa.singular_values_ \n",
        "V_transpose = lsa.components_.T"
      ],
      "metadata": {
        "id": "nOHUYZkZOPss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the topics with their terms\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "terms = tfidf.get_feature_names_out()\n",
        "\n",
        "# Bobot setiap topik terhadap  dokumen\n",
        "for index, component in enumerate(lsa.components_):\n",
        "    zipped = zip(terms, component)\n",
        "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
        "    top_terms_list=list(dict(top_terms_key).keys())\n",
        "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
      ],
      "metadata": {
        "id": "sXqOCILOORVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}